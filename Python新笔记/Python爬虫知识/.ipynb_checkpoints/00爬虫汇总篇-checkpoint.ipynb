{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫- 页面获取\n",
    "- python爬虫包：urllib, urllib3, httplib2,requests,urllib和requests配合使用\n",
    "# urllib\n",
    "- 包含模块：\n",
    "    - urllib.request:打开的读取urls\n",
    "    - urllib.error: 包含urllib.request产生的常见的错误，使用try捕捉\n",
    "    - urllib.parse: 包含解析url的方法\n",
    "    - urllib.robotparse: 解析robots.txt文件\n",
    "- 爬取的网页的编码问题解决\n",
    "    - chardet 可以自动检测页面文件的编码格式，但不一定准确\n",
    "- urlopen返回对象\n",
    "    - 参数\n",
    "    - geturl():返回请求对象的url\n",
    "    - info:请求反馈对象的meta信息\n",
    "    - getcode：返回http code\n",
    "- request.date\n",
    "    - 访问网络的两种方法\n",
    "        - get：利用参数给服务器传递信息\n",
    "            - 参数为dict，然后用parse编码\n",
    "        - post\n",
    "            - 一般向服务器传递参数使用\n",
    "            - post是自动把信息自动加密处理\n",
    "            - 想使用post就需要用到urlopen的data参数\n",
    "            - 使用post意味着http的请求头需要更改为如下：\n",
    "                - Content-Type:application/x-www.form-urlencode\n",
    "                - Content-Length:数据长度\n",
    "                - 简而言之，一旦更改请求方法，请注意其它请求头信息相适应\n",
    "            - urllib..parse.urlencode可以将字符串自动转换成上面的\n",
    "            - 为了更多的设置请求信息，urlopen不够用，需要用request.Requeset类\n",
    "- urllib.error\n",
    "    - URLError产生的原因：\n",
    "        - 没网\n",
    "        - 服务器链接失败\n",
    "        - OSerror的子类\n",
    "        - 不知道指定服务器\n",
    "    - HTTPError，是URLError的一个子类    \n",
    "    - 两者区别\n",
    "        - HTTPError是对应的HTTP请求的返回错误码，如果返回错误码是400以上的，则引发HTTPError\n",
    "        - URLError对应的一般是网咯出现问题，包括url问题\n",
    "        - 关系区别：OSError-URLError-HRRPError\n",
    "- UserAgent\n",
    "    - UserAgent:用户代理，简称UA，属于heads的一部分，服务器通过UA来判断访问中身份\n",
    "    - 常见的UA值，使用的时候可以直接复制黏贴，也可以用浏览器访问的时候抓包\n",
    "- 设置UA可以通过两种方式：\n",
    "    - headers\n",
    "    - add_header\n",
    "- ProxyHandler处理（代理服务器）  \n",
    "    - 使用代理IP，是爬虫的常用手段\n",
    "    - 获取代理服务器的地址\n",
    "        - 西刺代理\n",
    "        - www.goubanjia.com\n",
    "    - 代理用来隐藏真实访问中，代理也不允许频繁访问某一个固定网站，代理一定要很多\n",
    "    - 代理使用步骤\n",
    "        1.设置代理地址\n",
    "        2.创建ProxyHandler\n",
    "        3.创建Opener\n",
    "        4.安装Opener\n",
    "- cookie & session\n",
    "    - session机制采用的是在服务器端保持状态的方案，由于http协议的无记忆性，为了弥补这个缺点，所采用的一个补充协议，session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能\n",
    "    - cookie机制采用的是在客户端保持状态的方案，发放给用户的一段信息\n",
    "- cookie和session的区别\n",
    "    - 存放位置不同\n",
    "    - cookie不安全\n",
    "    - session会保存在服务器上一定时间\n",
    "    - 单个cookie保存数据不超过4k，很多浏览器限制一个站点最多保存20个\n",
    "- session的存放位置\n",
    "    - 存在服务器端\n",
    "    - 一般情况，session是放在内存中或者数据库中\n",
    "- cookie登录举例\n",
    "    - 可以将cookie手动写入headers\n",
    "    - http模块包含一些关于cookie的模块,通过他们我们可以自动使用cookie\n",
    "        - CookieJar(保存在内存中\n",
    "            - 管理存储cookie，向传出的http请求添加cookie\n",
    "            - cookie存储在内存中，cookieJar实例回收后cookie将消失\n",
    "        - FileCookieJar(filename, delayload=None, policy=None):（保存在文件夹中）\n",
    "            - 使用文件管理cookie\n",
    "        - MoaillaCookieJar\n",
    "            - 创建与mocilla浏览器cookie.txt兼容的FileCookieJar实例\n",
    "        - LwpCookieJar\n",
    "            - 创建与libwww-perl标准兼容的Set-Cookie3格式的FileCookieJar\n",
    "        - 他们的关系是CookieJar->FileCookieJar->MozillaCookieJar & LwpCookieJar\n",
    "    - 利用cookie自动登录流程\n",
    "        - 打开登录页面后自动通过用户名密码登录\n",
    "        - 自动提取反馈回来的cookie\n",
    "        - 利用提取的cookie登录隐私页面\n",
    "    - Handler是Handler的实习\n",
    "        - 用来处理复杂的请求\n",
    "            - 生成cookie的管理器\n",
    "               cookie_handler = request.HTTPCookieProcessor(cookie)\n",
    "            - 创建http请求管理器\n",
    "                http_handler = request.HTTPHandler()\n",
    "            - 生成HTTPS管理器\n",
    "                https_handler = request.HTTPSHandler()\n",
    "- 创立Handler后，使用opener打开，打开后相应的业务有相应的handler处理\n",
    "- cookie作为一个变量，打印处理\n",
    "    - cookie的属性\n",
    "        - name\n",
    "        - value：值\n",
    "        - domain：可以访问此cookie的域名\n",
    "        - path：可以访问cookie的页面路径\n",
    "        - expires：过期时间\n",
    "        - size：大小\n",
    "        - Http字段\n",
    "- cookie的保存为FileCookieJar\n",
    "- cookie的读取\n",
    "# SSL\n",
    "    - ssl证书就是指遵守ssl安全套阶层协议的服务器数字证书（）\n",
    "    - 遇到不信任的ssl证书，需要单独处理\n",
    "- js加密\n",
    "    - 有的反爬虫策略采用js对需要传输的数据进行加密处理（同常识md5值）\n",
    "    - 经过加密，传输的就是密文，\n",
    "    - 但是加密函数或者过程一定是在浏览器完成，也就是一定会把代码（js代码）暴露给使用者\n",
    "    - 通过阅读加密算法，就可以模拟出加密过程，从而达到破解\n",
    "    - 把浏览器中的js想办法找出来，仔细观察\n",
    "# ajax\n",
    "    - 异步请求\n",
    "    - 一定会有url，请求方法，可能有数据\n",
    "# Requests\n",
    "- 更加简洁友好\n",
    "- 底层使用的是urllib3\n",
    "- get请求\n",
    "    - request.get(url)\n",
    "    - request.request(\"get\",url)\n",
    "    - 可以带有headers和parmas参数\n",
    "- post\n",
    "    - rsp - requests.post(url, data=data)\n",
    "- proxy\n",
    "    - 代理有可能报错，如果使用人数过多，则会被强行关闭\n",
    "        proxies = {\n",
    "        \"http\":address of proxy,\n",
    "        \"https\":address of proxy,\n",
    "        }\n",
    "        rsp = requests.request(\"get\", \"http:xxxxxxx\", proxies=proxies)\n",
    "- 用户代理\n",
    "    - 代理验证\n",
    "            可能需要使用HTTP basix Auth,可以这样\n",
    "            #格式为：    用户名:密码@代理地址:端口地址\n",
    "            proxy = {\"http\": \"china:123456@192.168.1.123:4444\"}\n",
    "            rsp = requests.get(\"http://baidu.com\", proxies=proxy)\n",
    "- web客户端验证\n",
    "    - 如果遇到web客户端验证，需要添加auth=(用户名,密码)\n",
    "            autu=(\"test1\", \"123456\"}#授权信息\n",
    "            rsp = requests.get(\"http://www.baidu.com\", auth=auth)\n",
    "            \n",
    "- cookie\n",
    "    - requests可以自动处理cookie信息\n",
    "                rsp = requests.get(\"http://xxxxxxx\")\n",
    "                # 如果对方服务器给传送过来cookie信息，则可以通过反馈的cookie属性得到\n",
    "                # 返回一个cookiejar实例\n",
    "                cookiejar = rsp.cookies\n",
    "                #则可以将cookiejar转换成字典\n",
    "                cookiedict = requests.utils.dict_from_cookiejar(cookiejar)\n",
    "- session\n",
    "    - 是客户端上的\n",
    "    - 模拟一次对话，从客户端浏览器链接服务器开始，到客户端浏览器断开\n",
    "    - 能让我们跨请求时保持某些参数，比如在同一个session实例发出的所有请求之间保持--- cookie\n",
    "            # 创建session对象，可以保持cookie值\n",
    "             ss = requests.session()\n",
    "             headers = {\"User-Agent\":\"xxxxxxxxxxxx\"}\n",
    "             data = {\"name\":\"xxxxxxxxxxx\"}\n",
    "             # 此时由创建的session管理请求，负责发出请求\n",
    "             ss.post(\"http://www.baidu.com\", data=data, headers=headers}\n",
    "             rsp = ss.get(\"xxxxxxxxxxx\")\n",
    "             \n",
    "- https请求验证ssl证书\n",
    "    - 参数verify负责表示是否需要验证ssl证书，默认是True\n",
    "    - 如果不需要验证ssl证书，则设置成False表示关闭\n",
    "            rsp = requests.get(\"http://www.baidu.com\", verify=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫- 页面数据提取\n",
    "- 结构化数据：先有结构再谈数据\n",
    "    - json文件\n",
    "        - JSON Path\n",
    "        - 转换成python类型进行操作\n",
    "    - XML文件\n",
    "        - 转换成python类型（xmltodict）\n",
    "        - Xpath\n",
    "        - CSS选择器\n",
    "        - 正则\n",
    "- 非结构化数据：先有数据，再谈结构\n",
    "    - 文本\n",
    "    - 电话号码\n",
    "    - 邮箱地址\n",
    "    - 处理此类数据一般采用正则表达式\n",
    "    - HTML文件\n",
    "        - 正则表达式\n",
    "        - Xpath\n",
    "        - CSS选择器\n",
    "# lxml\n",
    "- 是xml和HTML的解析器\n",
    "- 功能：\n",
    "    - 解析HTML（补全代码）\n",
    "    - 文件读取，只能读格式为xml的文件\n",
    "    - etree和Xpath的配合使用\n",
    "# CSS选择器  BeautifulSoup4\n",
    "- 不懂就找官网读文档\n",
    "- 几个常用爬取工具的比较\n",
    "    - 正则：快，但不好用\n",
    "    - beautifulSoup4：慢，使用简单\n",
    "    - lxml:比较快\n",
    "- 四大对象\n",
    "    - Tag\n",
    "    - NavigableString\n",
    "    - BeautifulSoup\n",
    "    - Comment\n",
    "- Tag\n",
    "    - 对应HTML中的标签\n",
    "    - 可以通过soup.tag_name，找到第一个就停止\n",
    "    - tag两个重要属性\n",
    "        - name\n",
    "        - attrs\n",
    "- NavigableString\n",
    "    - 对应内容值\n",
    "- BeautifulSoup   \n",
    "    - 表示的是一个文档的内容，大部分可以把它当做tag对象\n",
    "    - 一般可以用soup来表示\n",
    "- Comment\n",
    "    - 特殊类型的NavagableString对象\n",
    "    - 对其输出，则内容不包括注释符号\n",
    "- 遍历文档对象\n",
    "    - contents：tag的子节点以列表的方式给出\n",
    "    - children：子节点以迭代器形式返回\n",
    "    - descendants：所有子孙节点\n",
    "    - string\n",
    "- 搜索文档对象\n",
    "    - find_all(name, attrs, recursive, text, ** kwargs)\n",
    "        - name:按照那个字符串搜索，可以传入的内容为\n",
    "            - 字符串\n",
    "            - 正则表达式\n",
    "            - 列表\n",
    "        - keyword参数：可以用来表示属性\n",
    "        - text：对应的tag属性\n",
    "- css选择器\n",
    "    - 使用soup.select，返回一个列表\n",
    "    - 通过标签名称：soup.select(\"title\")\n",
    "    - 通过名称：soup.select(\".content\")\n",
    "    - id查找：soup.select(\"#name_id\")\n",
    "    - 组合查找：soup.select(\"div #input_content\")\n",
    "    - 属性查找：soup.select(\"img[class='photo'])\n",
    "    - 获取tag内容： tag.get_text\n",
    "    \n",
    "# 正则表达式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态HTML\n",
    "## 爬虫与反爬虫\n",
    "## 动态HTML介绍\n",
    "- JavaScript\n",
    "- JQuery\n",
    "- ajax\n",
    "- DHTML\n",
    "- python采集动态数据\n",
    "    - 从JavaScript代码入手采集，比较麻烦\n",
    "    - python第三方库运行JavaScript，直接采集在浏览器看到的页面\n",
    "\n",
    "## selenium + PhantomJS\n",
    "- Selenium:web自动化测试工具\n",
    "    - 自动加载页面\n",
    "    - 获取数据\n",
    "    - 截屏\n",
    "    - 安装：pip install selenim==2.48.0\n",
    "- Selenium操作主要分为两大类\n",
    "    - 得到UI元素\n",
    "        - find_element_by_id\n",
    "        - find_delements_by_name\n",
    "        - find_elements_by_xpath\n",
    "        - find_elements_by_link_text\n",
    "        - find_elements_by_partial_link_text\n",
    "        - find_elements_by_tag_name\n",
    "        - find_elements_by_class_name\n",
    "        - find_elements_by_css_selector\n",
    "    - 基于UI元素操作的模拟\n",
    "        - 单一\n",
    "        - 右键\n",
    "        - 拖曳\n",
    "        - 输入\n",
    "        - 可以通过导入ActionsChains类来做到\n",
    "- PhantomJS(幽灵浏览器)\n",
    "    - 基于Webkit的无界面浏览器\n",
    "- Selenium库有一个webDriver的API\n",
    "- webDiver可以跟页面上的元素进行各种交互，用它可以进行爬取\n",
    "\n",
    "### 如何在scrapy中使用selenium\n",
    "- 可以放入中间体中的process_request函数中\n",
    "- 在函数中调用selenium，完成爬取后返回response\n",
    "         实例：在middlewares.py文件中添加如下的信息\n",
    "         class MyUserAgentMiddleware(object):\n",
    "             def process_request(.......):\n",
    "                 driver = webdriver.Chrome()\n",
    "                 html = driver.page_source\n",
    "                 driver.quit()\n",
    "                 retrurn HtmlResponse(url=request.url, encoding='utf-8',body=html,request=request)\n",
    "                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy\n",
    "# 爬虫框架\n",
    "- scapy框架介绍\n",
    "    - 读文档!        \n",
    "- scrapy概述\n",
    "    - 包含各个部件\n",
    "        - ScrapyEngine：神经中枢\n",
    "        - Scheduler调度器：接收引擎发来的request请求，并把他们入队，以便后续的调度\n",
    "        - Downloader下载器：负责抓取网页，并把得到的response传给spider\n",
    "        - Spider爬虫：解析response，得到items和urls\n",
    "        - ItemPipeline管道：详细处理item，负责清洗，验证等\n",
    "        - DownloaderMiddlerware下载中间体：处理传送到下载器的request和传送到引擎的response\n",
    "        - SpiderMiddlerware爬虫中间体：处理抓取器的输入和输出\n",
    "        \n",
    "- scrapy中的三个主要参数\n",
    "    - scrapy.Request()\n",
    "        - 参数有：url,callback,method='GET',headers,body,cookies,meta,encoding,priority=0(优先级),dont_filter=False(是否去重)，errback等。meta可以用来传递参数，在request中定义meta如meta={'item': item},然后下一个类的response调用如item=response.meta['item']\n",
    "    - response的参数有：urljoin\n",
    "    - 日志：self.logger.info(\"visited success\"):记录日志\n",
    "    \n",
    "- 爬虫项目大概流程\n",
    "    - 新建项目：scrapy startproject xxx\n",
    "    - 定义所要抓取的爬虫的项目：名字.py，名字在工程中必须要独一无二\n",
    "    - 制作爬虫：\n",
    "    - 编写和配置ItemPipeline\n",
    "    - 执行爬虫\n",
    "        第四步并非一定要执行\n",
    "      实例 import scrapy                                                       简便写法\n",
    "            class QuotesSpider(scrapy.Spider):\n",
    "                name = \"quotes\"\n",
    "                def start_requests(self):                                      class QuotesSpider(scrapy.Spider):\n",
    "                    urls = [                                                         name = \"quotes\"\n",
    "                        'http://quotes.toscrape.com/page/1/',                        start_urls = [\n",
    "                        'http://quotes.toscrape.com/page/2/',                               'http://quotes.toscrape.com/page/1/',\n",
    "                    ]                                                                       'http://quotes.toscrape.com/page/2/',\n",
    "                    for url in urls:                                                   ]\n",
    "                        yield scrapy.Request(url=url, callback=self.parse)           def parse(self, response):\n",
    "                                                                                           page = response.url.split(\"/\")[-2]\n",
    "                def parse(self, response):                                                 filename = 'quotes-%s.html' % page\n",
    "                    page = response.url.split(\"/\")[-2]                                     with open(filename, 'wb') as f:\n",
    "                    filename = 'quotes-%s.html' % page                                            f.write(response.body)\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(response.body)\n",
    "                    self.log('Saved file %s' % filename)      \n",
    "\n",
    "\n",
    "                                \n",
    "## Spider\n",
    "- 属性：\n",
    "    - name:设置爬虫名称，要求唯一\n",
    "    - start_urls:设置第一批爬取的url\n",
    "    - allow_domains：设置spider允许爬的域名列表\n",
    "    - start_request(self):读取start_urls并开始循环\n",
    "    - custom_settings：个性化设置，会覆盖全局的设置\n",
    "    - crawler:抓取器，与spider绑定在一起\n",
    "    - logger：日志实例\n",
    "- 函数方法：\n",
    "    - from_crawler(cralwer,*args,**kwrgs):类方法，用于创建spides\n",
    "    - start_requests():生成初始的requests\n",
    "    - maker_requests_from_url(url):根据url生成一个request\n",
    "    - parse（response）:回调解析函数，对request获得的response进行解析，用来解析网页内容\n",
    "    - self.logger.info(\"visited success\"):记录日志\n",
    "    - closed(reason):当spider关闭的时候调用\n",
    "- 子类介绍\n",
    "    - CrawlSpider：最常用的spider，增加了两个成员：\n",
    "        rules:定义了一些抓取规则，怎么跟踪，使用哪一个parse函数：parse_start_url(response)：解析初始url的相应response\n",
    "            import scrapy\n",
    "            from scrapy.spiders import CrawlSpider, Rule\n",
    "            from scrapy.linkextractors import LinkExtractor\n",
    "            class MySpider(CrawlSpider):\n",
    "                name = 'example.com'\n",
    "                allowed_domains = ['example.com']\n",
    "                start_urls = ['http://www.example.com']\n",
    "                rules = (\n",
    "                    # Extract links matching 'category.php' (but not matching 'subsection.php')\n",
    "                    # and follow links from them (since no callback means follow=True by default).\n",
    "                    Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))),\n",
    "                    # Extract links matching 'item.php' and parse them with the spider's method parse_item\n",
    "                    Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'),\n",
    "                )\n",
    "                def parse_item(self, response):\n",
    "                    self.logger.info('Hi, this is an item page! %s', response.url)\n",
    "                    item = scrapy.Item()\n",
    "                    item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)')\n",
    "                    item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').get()\n",
    "                    item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').get()\n",
    "                    item['link_text'] = response.meta['link_text']\n",
    "                    return item\n",
    "               \n",
    "## Items\n",
    "- items：保存爬取数据的容器，和字典类似，所要保存的任何内容，都要需要使用item来定义\n",
    "   代码在item.py中  import scrapy\n",
    "                    class Product(scrapy.Item):\n",
    "                        name = scrapy.Field()\n",
    "                        price = scrapy.Field()\n",
    "                        last_updated = scrapy.Field(serializer=str) #非必须\n",
    "                        \n",
    "- ItemPipeline\n",
    "    - 爬虫提取出数据存入item后，item中保存的数据需要进一步处理，比如清洗，去重，存储，查重等\n",
    "    - process_item(self, item, spider)：\n",
    "        - item (Item 对象或者一个dict)指的是被爬取的item。被丢弃的item不会被之后的pipeline处理\n",
    "        - 此方法必须实现\n",
    "        - spider (Spider 对象) – 爬取该item的spider\n",
    "    - __init__:构造函数\n",
    "        - 进行一些参数的初始化\n",
    "    - open_spider(spider)\n",
    "        - spider对象被开启的时候调用\n",
    "    - close_spider(spider):\n",
    "        - 当spider对象被关闭的时候调用\n",
    "    - from_crawler(cls, crawler)\n",
    "        - 如果给出，这个类方法将会被调用从Crawler创建一个pipeline实例，它必须返回一个pipeline的新的实例，Crawler对象提供了调用scrapy所有的核心组件的权限，比如你可以调用settings里面的设置项。\n",
    "        实例去重：from scrapy.exceptions import DropItem\n",
    "                    class DuplicatesPipeline(object):\n",
    "                        def __init__(self):\n",
    "                            self.ids_seen = set()\n",
    "                        def process_item(self, item, spider):\n",
    "                            if item['id'] in self.ids_seen:\n",
    "                                raise DropItem(\"Duplicate item found: %s\" % item)\n",
    "                            else:\n",
    "                                self.ids_seen.add(item['id'])\n",
    "                                return item\n",
    "                                \n",
    "## Feed_exports\n",
    "- 存储抓取的结果\n",
    "\n",
    "## 中间件\n",
    "    - 处于引擎和下载器中间的一层组件\n",
    "    - 可以有很多个，按顺序加载执行\n",
    "    - 作用是对发出的请求和返回的结果进行预处理\n",
    "    - 在middlewares文件中\n",
    "    - 需要在settings中设置以生效\n",
    "    - 一个中间件完成一项功能\n",
    "    - 必须实现以下一个或多个方法\n",
    "        - process_request(self, request, spider)\n",
    "            - 在request通过的时候必须被调用\n",
    "            - 必须返回None或Response或Request或raise IgnoreRequest其中之一\n",
    "            - None：scrapy将继续处理该request\n",
    "            - Request：scrapy会停止调用process_request并重新调度返回的request\n",
    "            - Response：scrapy将不会调用其它的process_request或process_exception，直接将response作为结果返回，同时会调用process_response函数\n",
    "        - process_response(self, request, response, spider)\n",
    "            - 跟process_request大同小异\n",
    "            - 每次返回结果的时候会自动调用\n",
    "            - 可以有多个，按顺序调用\n",
    "            \n",
    "- 去重\n",
    "    - 为了防止爬虫陷入死循环，需要去重\n",
    "    - 即在spider中的parse函数中，返回request的时候加上dont_filter=False参数\n",
    "        - ......scrapy.Request(url=url, dont_filter=False)\n",
    "                 \n",
    "\n",
    "                         \n",
    "# scrapy shell + url(输出一系列可用的对象和函数)\n",
    "- scrapy crawl quotes开始爬虫\n",
    "- scrapy crawl quotes -o quotes.json,开始爬虫并将文件存贮为json文件，一旦执行两次且第一次文件没删除则json文件就会出现乱码,scrapy crawl quotes -o quotes.jl，这个就不会出现乱码\n",
    "### 爬取多页网页的方法\n",
    "            import scrapy\n",
    "            class QuotesSpider(scrapy.Spider):\n",
    "                name = \"quotes\"\n",
    "                start_urls = [\n",
    "                    'http://quotes.toscrape.com/page/1/',\n",
    "                ]\n",
    "                def parse(self, response):\n",
    "                    for quote in response.css('div.quote'):\n",
    "                        yield {\n",
    "                            'text': quote.css('span.text::text').get(),\n",
    "                            'author': quote.css('small.author::text').get(),\n",
    "                            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "                        }\n",
    "                    next_page = response.css('li.next a::attr(href)').get()\n",
    "                    if next_page is not None:\n",
    "                        next_page = response.urljoin(next_page) # urljoin()是保证路径是绝对路劲\n",
    "                        yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "- 爬取返回得到的response解析\n",
    "- xpath\n",
    "- css:方法\n",
    "    有get(),getall(),get只返回找到的第一个，getall()是返回一个列表\n",
    "    re()返回一个列表，re_first()返回第一个\n",
    "    extract_first()和extract()同get()、getall()功能相同，感觉要被废除了\n",
    "    - 实例\n",
    "            response.xpath('//base/@href').get()\n",
    "            response.css('base::attr(href)').get()\n",
    "            response.css('base').attrib['href']\n",
    "            response.css('base::href').re(r\"'http([\\W\\w]+)'\")\n",
    "       上述结果 'http://example.com/'\n",
    "\n",
    "            response.xpath('//a[contains(@href, \"image\")]/img/@src').getall()\n",
    "            response.css('a[href*=image] img::attr(src)').getall()\n",
    "       上述结果  ['image1_thumb.jpg',\n",
    "                     'image2_thumb.jpg',\n",
    "                     'image3_thumb.jpg',\n",
    "                     'image4_thumb.jpg',\n",
    "                     'image5_thumb.jpg']\n",
    "- response.body：网页代码\n",
    "- response.headers是返回的http头信息\n",
    "                         \n",
    "- settings的优先级\n",
    "Command line options(most precedence)    命令行选项(最优先)命令行选项(最优先级)\n",
    "Settings per-spider                      设置per-spider\n",
    "Project settings module                  项目设置模块\n",
    "Default settings per-command             各命令默认设置\n",
    "Default global settings(less precedence) 默认全局设置(优先级较低)\n",
    "\n",
    "# 分布式爬虫\n",
    "- 单机爬虫的问题\n",
    "    - 单机效率\n",
    "    - IO吞吐量\n",
    "- 多爬虫问题\n",
    "    - 数据共享\n",
    "    - 在空间不同的多态机器，可以称为分布式\n",
    "- 需要改善\n",
    "    - 共享队列，去重\n",
    "- 解决：Redis\n",
    "    - 内存数据库\n",
    "    - 同时可以落地保存到硬盘\n",
    "    - 可以去重\n",
    "    - 可以把它理解成一个共dict，set，list的集合体\n",
    "    - 可以对保存的内容进行生命周期控制\n",
    "- 内容保存数据库\n",
    "    - MongoDB\n",
    "    - Mysql等传统数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy实现user-Agent和proxy伪装的代码\n",
    "import random \n",
    "import base64\n",
    "    # 从settings设置文件中导入值\n",
    "from settings import USER_AGENT\n",
    "from settings import PROXIES\n",
    "\n",
    "    # 随机的Useer_Agent\n",
    "class RandomUserAgent(object):\n",
    "    def process_request(self, request, spider):\n",
    "        useragent = randon.choice(USER_AGENT)\n",
    "        request.headers.setdefault(\"User-Agent\", useragent)\n",
    "class RandomProxy(object):\n",
    "    def process_request(self, request, spider):\n",
    "        proxy = random.choice(PROXIES)\n",
    "        if proxy['uesr_passwd'] is None:\n",
    "            # 没有代理账户验证的代理使用方式\n",
    "            request.meta['proxy'] = \"http://\" + proxy['ip_port']\n",
    "        else:\n",
    "            # 对账户密码进行base64编码转换\n",
    "            request.headers['Proxy-Authorization'] = 'Basic' + base64_userpasswd\n",
    "            request.meta['proxy'] = \"http://\" + proxy['ip_port']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy实现user-Agent和proxy伪装的代码举例\n",
    "\n",
    "# user-Agent用户设置\n",
    "## 第一步：在settings.py文件中添加如下的信息。（useragent可以自己找）\n",
    "    MY_USER_AGENT = [\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "        ]\n",
    "## 第二步：在middlewares.py文件中添加如下的信息\n",
    "    import scrapy\n",
    "    from scrapy import signals\n",
    "    from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n",
    "    import random\n",
    "\n",
    "    class MyUserAgentMiddleware(UserAgentMiddleware):\n",
    "        def __init__(self, user_agent):\n",
    "            self.user_agent = user_agent\n",
    "\n",
    "        @classmethod\n",
    "        def from_crawler(cls, crawler):\n",
    "            return cls(\n",
    "                user_agent=crawler.settings.get('MY_USER_AGENT')\n",
    "            )\n",
    "\n",
    "        def process_request(self, request, spider):\n",
    "            agent = random.choice(self.user_agent)\n",
    "            request.headers['User-Agent'] = agent\n",
    "        \n",
    "## 第三步：将我们自定义的这个MyUserAgentMiddleware类添加到DOWNLOADER_MIDDLEWARES，像下面这样\n",
    "    DOWNLOADER_MIDDLEWARES = {\n",
    "        'scrapy.downloadermiddleware.useragent.UserAgentMiddleware': None, \n",
    "        'myproject.middlewares.MyUserAgentMiddleware': 400,\n",
    "    }\n",
    "\n",
    "# 设置随机代理proxy\n",
    "## 第一步：在settings.py文件中添加如下的信息。（需要自己找代理IP）\n",
    "    PROXIES = ['http://183.207.95.27:80', 'http://111.6.100.99:80', 'http://122.72.99.103:80', \n",
    "               'http://60.31.239.166:3128', 'http://114.55.31.115:3128', 'http://202.85.213.220:3128']\n",
    "## 第二步：在middlewares.py文件中，添加下面的代码。\n",
    "    import scrapy\n",
    "    from scrapy import signals\n",
    "    import random\n",
    "\n",
    "    class ProxyMiddleware(object):\n",
    "        def __init__(self, ip):\n",
    "            self.ip = ip\n",
    "\n",
    "        @classmethod\n",
    "        def from_crawler(cls, crawler):\n",
    "            return cls(ip=crawler.settings.get('PROXIES'))\n",
    "\n",
    "        def process_request(self, request, spider):\n",
    "            ip = random.choice(self.ip)\n",
    "            request.meta['proxy'] = ip\n",
    "        \n",
    "## 第三步：将我们自定义的这个MyUserAgentMiddleware类添加到DOWNLOADER_MIDDLEWARES，像下面这样\n",
    "    DOWNLOADER_MIDDLEWARES = {\n",
    "        'myproject.middlewares.ProxyMiddleware': 543,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy模拟登陆网站\n",
    "        import scrapy\n",
    "\n",
    "        class LoginSpider(scrapy.Spider):\n",
    "            name = \"myproject\"\n",
    "            start_urls = ['http://www.baidu.com/user/login.php']\n",
    "            def parse(self, response):\n",
    "                 # scapy.FormRequest是scrapy.Request的子类\n",
    "                return scrapy.FormRequest.from_response(  \n",
    "                    response,formdata={'username':'john','password':'123'},callback=self.after_login\n",
    "                ) \n",
    "            def after_login(self, response):\n",
    "                if \"authentication failed\" in response.body:\n",
    "                    self.logger.error(\"Login falied\")\n",
    "                    return \n",
    "                # 后又继续抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy爬取的数据存储到MySQL方法\n",
    "## 第一步，在数据库先生成一个表，设置表格属性，具体参见https://www.runoob.com/mysql/mysql-create-tables.html\n",
    "    比如  CREATE TABLE IF NOT EXISTS `runoob_tbl`(\n",
    "               `runoob_id` INT UNSIGNED AUTO_INCREMENT,\n",
    "               `runoob_title` VARCHAR(100) NOT NULL,\n",
    "               `runoob_author` VARCHAR(40) NOT NULL,\n",
    "               `submission_date` DATE,\n",
    "               PRIMARY KEY ( `runoob_id` )\n",
    "            )ENGINE=InnoDB DEFAULT CHARSET=utf8;\n",
    "## 第二步：item文件相关设置：具体参见https://www.jianshu.com/p/ad7ba01b0e77\n",
    "### 1.先封装items.py文件\n",
    "    如：import scrapy\n",
    "        class XCProxyItem(scrapy.Item):\n",
    "            # define the fields for your item here like:\n",
    "            # name = scrapy.Field()\n",
    "            IP = scrapy.Field()\n",
    "\n",
    "### 2.在自己的项目文件中导入自己编写的item文件并运用\n",
    "    如：  from myproject.items import XCProxyItem\n",
    "            def parse(self, response):\n",
    "            ip = response.xpath(\"//table[@id='ip_list']\")\n",
    "                pre_item = XCProxyItem()\n",
    "                pre_item['IP'] = ip.xpath('td[2]/text()')[0].getall()\n",
    "                yield pre_item\n",
    "                \n",
    "## 第三步：设置pipelines文件以连接MySQL\n",
    "    代码1实例   import MySQLdb\n",
    "                from twisted.enterprise import adbapi\n",
    "                import MySQLdb.cursors\n",
    "\n",
    "                class XCPipeline(object):\n",
    "                    def __init__(self, dbpool):\n",
    "                        self.dbpool = dbpool\n",
    "\n",
    "                    @classmethod\n",
    "                    def from_settings(cls, settings):\n",
    "                        # 设置登录MySQL的相关信息\n",
    "                        dbparams = dict(\n",
    "                            host=settings['MYSQL_HOST'],\n",
    "                            db=settings['MYSQL_DBNAME'],\n",
    "                            user=settings['MYSQL_USER'],\n",
    "                            password=settings['MYSQL_PASSWD'],\n",
    "                            charset='utf8',\n",
    "                            cursorclass=MySQLdb.cursors.DictCursor,\n",
    "                            use_unicode=False,\n",
    "                        )\n",
    "                        dbpool = adbapi.ConnectionPool('MySQLdb', **dbparams)\n",
    "                        return cls(dbpool)\n",
    "\n",
    "                    def process_item(self, item, spider):\n",
    "                        query = self.dbpool.runInteraction(self._conditional_insert, item)\n",
    "                        query.addErrback(self._handle_error, item, spider)\n",
    "                        return item\n",
    "\n",
    "                    def _conditional_insert(self, tx, item):\n",
    "                        sql = (\"insert into runoob_tbl(IP)\" #runoob_tbl是你自己建立的表格\n",
    "                               \"values(%s)\")\n",
    "                        params = (item['IP'])\n",
    "                        tx.execute(sql, params)\n",
    "\n",
    "                    def _handle_error(self, failure, item, spider):\n",
    "                        print(failure)\n",
    "                        \n",
    "    代码二实例：（自我感觉代码更简单）\n",
    "            import MySQLdb\n",
    "        from twisted.enterprise import adbapi\n",
    "        import MySQLdb.cursors\n",
    "\n",
    "        class XCPipeline(object):\n",
    "            def __init__(self):\n",
    "                self.connect = MySQLdb.connect(\n",
    "                    host='localhost',\n",
    "                    port=3306,\n",
    "                    user='root',\n",
    "                    passwd='lt123456',\n",
    "                    db='proxy',\n",
    "                    charset='utf8'\n",
    "                )\n",
    "                self.cursor = self.connect.cursor()\n",
    "\n",
    "            def process_item(self, item, spider):\n",
    "                insert_sql = \"insert into runoob_tbl(IP,PORT,TYPE,POSITION,SPEED,LAST_CHECK_TIME) values(%s,%s,%s,%s,%s,%s)\"\n",
    "                params = (item['IP'], item['PORT'], item['POSITION'], item['TYPE'], item['SPEED'], item['LAST_CHECK_TIME'])\n",
    "                self.cursor.execute(insert_sql,params)\n",
    "                self.connect.commit()\n",
    "                \n",
    "            def close_spider(self, spider):\n",
    "                self.cursor.close()\n",
    "                self.connect.close()\n",
    "                        \n",
    "# 第四步：设置settings\n",
    "    匹配代码一实例：\n",
    "        ROBOTSTXT_OBEY = False\n",
    "        DOWNLOAD_DELAY = 0.25 #延迟以模拟机器人\n",
    "        ITEM_PIPELINES = {\n",
    "            'myproject.pipelines.XCPipeline': 300,\n",
    "        }\n",
    "        MYSQL_HOST = '127.0.0.1'\n",
    "        MYSQL_DBNAME = 'proxy'      #在你所要导入的数据库名字，请修改\n",
    "        MYSQL_USER = 'root'         #数据库账号\n",
    "        MYSQL_PASSWD = 'lt123456'   #数据库密码\n",
    "        MYSQL_PORT = 3306       #数据库端口，默认不改\n",
    "        \n",
    "    匹配代码二实例：\n",
    "        ROBOTSTXT_OBEY = False\n",
    "        DOWNLOAD_DELAY = 0.25 #延迟以模拟机器人\n",
    "        ITEM_PIPELINES = {\n",
    "           'brand_new_spider.pipelines.XCPipeline': 300,\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
