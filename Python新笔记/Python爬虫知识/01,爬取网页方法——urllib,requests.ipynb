{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP\n",
    "- 爬虫实际上就是程序模拟人向网址发起请求\n",
    "- 用户在浏览器中发送一个HTTP请求的过程\n",
    "    - 用户输入网址url后，浏览器会用HTTP服务器发送HTTP请求（request请求）获取对应网址的HTML文件，服务器把Response文件对象发送给浏览器：请求分为Get和Post两种\n",
    "    - 浏览器分析系Response中的HTML，里面有很多文件如：images，css，js文件，浏览器又再次发送request去获取图片，css，js文件\n",
    "    - 当所有的文件都下载成功后，网页会根据HTML语法文档，将页面完整显示出来\n",
    "\n",
    "# HTTP请求方法\n",
    "- HTTP1.0中定义了三种请求方法:\n",
    "    - GET, POST, HEAD\n",
    "        GET\t请求指定的页面信息，并返回实体主体。\n",
    "        HEAD\t类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头\n",
    "        POST\t向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。\n",
    "- HTTP1.1新增了六种请求方法\n",
    "    - OPTIONS, PUT, PATCH, DELETE, TRACE, CONNECT\n",
    "        PUT\t从客户端向服务器传送的数据取代指定的文档的内容。\n",
    "        DELETE\t请求服务器删除指定的页面。\n",
    "        CONNECT\tHTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。\n",
    "        OPTIONS\t允许客户端查看服务器的性能。\n",
    "        TRACE\t回显服务器收到的请求，主要用于测试或诊断。\n",
    "        PATCH\t是对 PUT 方法的补充，用来对已知资源进行局部更新 。\n",
    "\n",
    "### url详解\n",
    "- URL：uniform resource locator :统一资源定位符\n",
    "- url组成；scheme://host:port/path/?query-string=xxx#anchor\n",
    "    - scheme:指定访问的协议：一般为http,https,ftp等\n",
    "    - host:主机名，域名\n",
    "    - post：端口号，http默认是80端口，https默认是443端口\n",
    "    - path：查找路径\n",
    "    - query-string:查询字符串\n",
    "    - anchor:锚点，后台一般不用管，前端用来做页面定位的\n",
    "- 在浏览器中请求一个url，浏览器会对这个url进行一个编码，除英文字母，数字和部分符号外，其余对象全部采用  百分号+十六进制 编码方式编码\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫简介\n",
    "- 定义：指按照一定的规则，自动的抓取万维网信息的程序或者脚本\n",
    "- 分类：通用爬虫，专用爬虫（聚焦爬虫）\n",
    "- Python爬虫需要用到的包\n",
    "    - python3.x:urllib和requests\n",
    "    \n",
    "## urllib模块\n",
    "- 包含模块：\n",
    "    - urllib.request:打开和读取urls\n",
    "    - urllib.error:包含urllib.request产生的常见的错误，使用try捕捉\n",
    "    - urllib.parse:包含解析URL的方法\n",
    "    - urllib.robotparse:解析robots.txt文件\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 请求数据的两种常用方法\n",
    "    - get:通常只从服务器获取数据，并不会对服务器资源产生任何影响的时候使用它。通常使用urllib.request.urlopen(url)这个就是get请求\n",
    "\n",
    "    - post:向服务器发送数据（登录)，上传文件等，会对服务器资源产生影响的时候。使用urllib.request.Request()和urlopen组合\n",
    "        - 使用post意味着HTTP的请求头需要更改应\n",
    "        - 利用字符串参数给服务器传递信息\n",
    "        - 参数在传入前类型为dict，使用urllib.parse.urlencode(dict)将dict转换成url编码类型，再传递给服务器\n",
    "    - 一般按它们的使用原则来选择使用get还是post，但有的网站有反爬虫机制，需要把get请求转换为post\n",
    "\n",
    "#### urllib.request模块\n",
    "- 导入from urllib import request\n",
    "- 方法：\n",
    "\n",
    "    - urlopen函数：rsp = request.urlopen(url):返回一个http.client.HTTPResponse对象,方法如下\n",
    "        - geturl():得到当初请求的url网址\n",
    "        - info():请求反馈对象的meta信息\n",
    "        - getcode():得到一个数字，表示HTTP code\n",
    "        - read(),readline(),readlines()\n",
    "        - urlopen得到的response对象是bytes格式，需要解码。为了解决这个问题，可以导入chardet,自动检测页面文件的编码格式，但是检测结果可能有误\n",
    "                导入：import chardet\n",
    "                cs = chardet.detect(),得到的参数为一个字典\n",
    "                cs.get(\"encoding\", \"utf-8\")把这句话放入页面解码中，比如content = html.decode(cs.get(\"encoding\", \"utf-8\")\n",
    "\n",
    "    - Request函数：rsp = urllib.request.Request()：同URLopen一样\n",
    "        - urllib.request.urlopen()不能够传递更多的请求头信息，用Request可以往里面放入等多的请求头信息，如data，headers等\n",
    "        \n",
    "    - urlretrieve()函数request.urltrieve(url, filename.html):将获取的网页保存到本地的文件中\n",
    "\n",
    "#### urllib.parse模块\n",
    "- 导入:from urllib import parse\n",
    "- 方法：\n",
    "    - parse.urlencode(data):将data编码成url编码格式\n",
    "    - parse_as():将url编码解码\n",
    "    - parse.urlparse(url)或parse.urlsplit():urlparse里面比urlsplit多一个params属性，对url进行分割,分成以下几部分：\n",
    "        - result.scheme得到协议类型\n",
    "        - resutl.netloc得到域名/主机名\n",
    "        - result.path得到路径\n",
    "        - result.query得到查询字符串\n",
    "\n",
    "\n",
    "### urllib.error\n",
    "    - urllib.error.URLError产生的原因：\n",
    "        - 没网\n",
    "        - 服务器连接失败\n",
    "        - 找不到指定的服务器，可能是url输入错误\n",
    "        - 是OSError的子类\n",
    "    - urllib.error.HTTPError是URLError的一个子类\n",
    "    - 两者的区别：\n",
    "        - HTTPError是对应的HTTP状态码中400以上的状态码对应的错误\n",
    "        - URLError：对应的是网络出现文献问题，包括url问题\n",
    "        - 继承关系：OSError-URLError-HTTPError\n",
    "        \n",
    "### 设置UserAgent\n",
    "- 方法：\n",
    "    - 构建名为header的字典，包含UserAgent\n",
    "    - add_header\n",
    "        - req = requeset.Request(url)\n",
    "        - req.add_header(\"User-Agent\":\"你的浏览器用户名\")\n",
    "    \n",
    "\n",
    "### ProxyHandler代理（代理服务器）\n",
    "- 使用代理IP，是爬虫的常用手段\n",
    "    - 获取代理服务器的地址\n",
    "        - www.xicidaili.com(西刺代理)，块代理，代理运\n",
    "    - 用IP地址来伪装时，量一定要大\n",
    "    - 基本使用步骤：\n",
    "        1.设置代理地址:proxy = {\"http\": \"49.70.99.15:9999\"}\n",
    "        2.创建ProxyHandler: handler = request.ProxyHandler(proxy)\n",
    "        3.创建Opener:opener=request.build_opener(proxy_handler)\n",
    "        4.安装Opener:request.install_opener(opener)\n",
    "        5.使用opener打开url：rsp = opener.open(url)\n",
    "        \n",
    "\n",
    "## cookie和session\n",
    "- cookie：为了解决HTTP协议的无状态性，所采用的一个补充协议\n",
    "- cookie是发放给用户（即HTTP浏览器）的一段信息，session是放在服务器的对应的另一半信息，用来记录用户信息\n",
    "- 两者的区别：\n",
    "     - cookie数据存放在客户的浏览器上，session数据放在服务器上。\n",
    "     - cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗   考虑到安全应当使用session。\n",
    "    - session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能   考虑到减轻服务器性能方面，应当使用COOKIE。\n",
    "     - 单个cookie保存的数据不超过4k，很多浏览器自己会限制一个站点最多保存20个\n",
    "     - 一般情况，session是放在内存中或放在数据库中\n",
    "        \n",
    "- 使用cookie登录；\n",
    "     - 手动复制：直接把cookie复制下来，手动放入请求头\n",
    "     - 自动导入：http模块包含一些cookiejar的模块，通过他们可以自动使用cookie\n",
    "         - CookieJar\n",
    "             - 管理存储cookie，向传出的HTTP请求添加cookie\n",
    "             - cookie存储在内存中，CookieJar实例回收后cookie将消失\n",
    "         - FileCookieJar(filename, delayload=None, policy=None)\n",
    "            - 使用文件管理cookie\n",
    "            - filename是保存cookie的文件\n",
    "         - MozillaCookieJar\n",
    "            - 创建与mocilla浏览器cookie.txt兼容的FileCookieJar实例\n",
    "         - LwpCookieJar\n",
    "            - 创建与libwww-perl标准兼容的Set-Cookie3格式的FileCookieJar实例\n",
    "         - 它们的关系是爷爷，儿子，两孙子\n",
    "- 自动使用cookie登录的流程是：\n",
    "    - 打开登录页面后自动通过用户名，密码登录\n",
    "    - 自动提取反馈回来的cookie\n",
    "    - 利用提取的cookie登录隐私页面\n",
    "- handler是Handler的实例\n",
    "    - 用来处理复杂请求\n",
    "        1、创建CookieJar的实例\n",
    "        cookie = cookiejar.CookieJar()\n",
    "        2、生成cookie的管理器\n",
    "        cookie_handler = request.HTTPCookieProcessor(cookie)\n",
    "        3、创建HTTP请求管理器\n",
    "        http_handler = request.HTTPHandler()\n",
    "        4、创建HTTPS请求管理器\n",
    "        https_handler = request.HTTPSHandler()\n",
    "        5、创建请求管理器\n",
    "        opener = request.build_opener(http_handler, https_handler, cookie_handler)\n",
    "    - 创建handler后，使用opener打开url，打开后相应的业务由相应的handler处理\n",
    "        - opener.open(url),此后cookie就保存在opener中，以后再登录就用opener.open(url),自动调用cookie\n",
    "    - cookie作为一个变量，打印出来\n",
    "        - cookie的格式：Set-Cookie: NAME=VALUE:Expires/Max-age=DATE;Path=PATH;Domain=DOMAIN; SECURE\n",
    "        - cookie的属性\n",
    "            - name：名称\n",
    "            - value：值\n",
    "            - domain：可以访问此cookie的域名\n",
    "            - path：可以访问此cookie的页面路径\n",
    "            - expires：过期时间\n",
    "            - size：大小\n",
    "            - SECURE:是否只在HTTPS协议下起作用\n",
    "    - cookie的保存：FileCookieJar\n",
    "    - cookie的读取：先获取，然后用另一台计算机伪装多次访问\n",
    "    \n",
    "# SSL\n",
    "- SSL证书就是指遵守ssl安全套阶层协议的服务器数字证书（SecuritySocketLayer）\n",
    "- CA是是数字证书认证中心，是发放，管理，废除数字证书的收信人的第三方机构\n",
    "- 遇到不信任的SSL证书需要单独处理（比如HTTP协议就是不安全的，HTTPS是安全的）\n",
    "        # 利用非认证的上下文环境替换认证的上下文环境import ssl\n",
    "            ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# js加密\n",
    "- js加密通常是取md5值\n",
    "- js加密这个过程一定在浏览器完成，也就是一定会把代码（js代码）暴露给使用者\n",
    "- 通过阅读加密算法，就可以模拟出加密过程，从而达到破解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ajax\n",
    "- 异步请求\n",
    "- 一定会有url请求方法，可能有数据\n",
    "- 一般使用json格式\n",
    "\n",
    "# requests模块:对人类非常友好\n",
    "- 更简洁更友好\n",
    "- requests适用于：\n",
    "        Keep-Alive & 连接池\n",
    "        国际化域名和 URL\n",
    "        带持久 Cookie 的会话\n",
    "        浏览器式的 SSL 认证\n",
    "        自动内容解码\n",
    "        基本/摘要式的身份认证\n",
    "        优雅的 key/value Cookie\n",
    "        自动解压\n",
    "        Unicode 响应体\n",
    "        HTTP(S) 代理支持\n",
    "        文件分块上传\n",
    "        流下载\n",
    "        连接超时\n",
    "        分块请求\n",
    "        支持 .netrc\n",
    "- 继承了Urllib的所有特征\n",
    "- 底层使用urllib3\n",
    "- 开源地址：https://github.com/requests/requests\n",
    "- 中文文档：https://requests.readthedocs.io/zh_CN/latest/\n",
    "- 方法：\n",
    "    - 请求方法：\n",
    "        - post, put, delete, head, options,get, request\n",
    "        - 其中最常用的是下面两种\n",
    "        - requests.get(url,param=,headers=，cookies=),param为字典类型，就是向网址传递你要查询的字符串。该方法返回一个response对象，\n",
    "            - 该对象的属性和方法有：\n",
    "                - url：返回访问的网址\n",
    "                - request:返回你请求服务器的方法\n",
    "                - status_code:响应状态码\n",
    "                - raise_for_status():抛出导致异常的问题\n",
    "                - headers:得到服务器返回的响应头\n",
    "                - cookies:若返回对象有cookies，则可以访问他\n",
    "                    - 若想发送cookies给服务器，设置cookies参数\n",
    "                - text:得到TXT格式内容\n",
    "                - encoding：得到内容的编码格式，可以修改它\n",
    "                - content：得到原文没经过任何编码\n",
    "                - json():用来处理json数据格式，前提是返回对象得是json格式\n",
    "            - param举例：比如在百度中搜索大熊猫，requests.get(url, param={\"wd\":\"大熊猫\"}\n",
    "            - 定制请求头时注意：定制header的优先级低于某些特定的信息源\n",
    "                - 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，``.netrc`` 的设置就无效了。\n",
    "                - 如果被重定向到别的主机，授权 header 就会被删除。\n",
    "                - 代理授权 header 会被 URL 中提供的代理身份覆盖掉。\n",
    "                - 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。\n",
    "        - requests.request(\"get\", url)：第一个参数表示请求方法\n",
    "        - requests.post(url, data=data):向服务器发送数据\n",
    "        \n",
    "#### 代理proxy\n",
    "        proxy = {\n",
    "            \"http\": \"address of procy\n",
    "            \"https\": address of proxy\n",
    "        }\n",
    "        rsp = requests.request(\"get\", url, proxies= proxy)\n",
    "- 代理有可能出错，当使用人数过多，可能会强行关闭\n",
    "\n",
    "- 用户验证\n",
    "    - 代理验证：可能需要使用Http basic Auth\n",
    "            #格式为：用户名:密码@代理地址:端口地址\n",
    "            proxy = { \"http\":\"China:123456@183.93.2.22:8889\"}\n",
    "            rsp = requests.get(url, proxies=proxy)\n",
    "- web客户端验证\n",
    "    - 如果遇到web客户端验证，需要添加auth = (用户名,密码)\n",
    "            auth=(username, password) #授权信息\n",
    "            rsp = requests.get(url, auth=auth)\n",
    "\n",
    "\n",
    "### cookie\n",
    "- requests可以自动处理cookies信息\n",
    "        rsp = requests.get(url)\n",
    "        #如果对方服务器传送过来cookie信息，则可以通过反馈的cookie属性得到\n",
    "        # 返回一个cookiejar实例\n",
    "        cookiejar = rsp.cookies\n",
    "        \n",
    "        # 可以将cookiejar转换成字典\n",
    "        cookiedict = requests.utils.dict_from_cookiejar(cookiejar)\n",
    "        或者cookiedict = rsp.cookie.get_dict()\n",
    "- session:\n",
    "    - 和服务器端的session完全不同\n",
    "    - 模拟一次会话，能从客户端浏览器连接服务器开始，到客户端浏览器断开\n",
    "    - 能让我们跨请求时保持某些参数，比如在同一个session实例发出的所有请求之间保持cookie\n",
    "    \n",
    "            # 创建session对象，可以保持cookie对象\n",
    "            ss = requessts.session()\n",
    "            headers = {xxxx}\n",
    "            data = {xxxx}\n",
    "            # 此时由创建的session管理请求的发出\n",
    "            ss.post(url, data=data, headers=headers)\n",
    "            \n",
    "\n",
    "### https请求验证ssl证书\n",
    "- 参数verify负责表示是否需要验证ssl证书，默认是True\n",
    "- 如果不需要验证ssl证书，则设置成False表示关闭\n",
    "        rsp = requests.get(url, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
